{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy intro\n",
    "### nlp object\n",
    "- contains processing pipeline\n",
    "- includes language-specific rules for tokenization, etc.\n",
    "### doc object\n",
    "- lets iterate over tokens\n",
    "### token object\n",
    "- represent tokens in a doc\n",
    "- provide various attributes like text, span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.lang.en.English'>\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "0 Hello\n",
      "1 world\n",
      "2 !\n",
      "3 $\n",
      "4 5\n",
      "5 ten\n",
      "6 .\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "world\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "world! $\n",
      "5 is_alpha: False\n",
      "5 is_punct: False\n",
      "5 is_like_num: True\n",
      "ten is_alpha: True\n",
      "ten is_punct: False\n",
      "ten is_like_num: True\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "print (type(nlp))\n",
    "doc = nlp(\"Hello world! $5 ten.\")\n",
    "print (type(doc))\n",
    "for token in doc:\n",
    "    print (str(token.i) + \" \" + token.text)\n",
    "token = doc[1]\n",
    "print (type(token))\n",
    "print (token.text) # text attribute\n",
    "span = doc[1:4] # span\n",
    "print (type(span))\n",
    "print (span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 is_alpha: False\n",
      "5 is_punct: False\n",
      "5 like_num: True\n",
      "ten is_alpha: True\n",
      "ten is_punct: False\n",
      "ten like_num: True\n"
     ]
    }
   ],
   "source": [
    "for i in range(4, 6):\n",
    "    token = doc[i]\n",
    "    print (token.text + \" is_alpha: \" + str(token.is_alpha))\n",
    "    print (token.text + \" is_punct: \" + str(token.is_punct))\n",
    "    print (token.text + \" like_num: \" + str(token.like_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "# Process the text\n",
    "doc = nlp(\"In 1990, more than 60% of people in East Asia were in extreme poverty. Now less than 4% are.\")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i+1]\n",
    "        # Check if the next token's text equals '%'\n",
    "        if next_token.text == '%':\n",
    "            print('Percentage found:', token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
