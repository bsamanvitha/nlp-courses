{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Datacamp's intro to NLP feature engineering course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "- text preprocessing (tokenization, lemmatization,)\n",
    "- text cleaning (stop words, punctuations, escape, special chars)\n",
    "Use cases relevantt to that application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readability tests\n",
    "- uses word, syllable, etc.\n",
    "- Example (in Engligh):\n",
    "    - Flesch reading ease\n",
    "    - Gunning fog index\n",
    "    - Simple measure of gobbledygook\n",
    "    - Dale-Chall score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flesch reading ease\n",
    "- harder to read if: \n",
    "- greater avg. sentences length \n",
    "- greater avg. syllables in a word\n",
    "\n",
    "- higher score = greater readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gunning fog index\n",
    "- % of complex words\n",
    "- higher score = lower readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.436002482929858, 20.735401069518716, 11.085587583148559, 5.926785009861934]\n",
      "[51.94945375543142, 36.318919786096274, 64.80227272727275, 87.7721143984221]\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/erinhengel/Textatistic\n",
    "forbes = '\\nThe idea is to create more transparency about companies and individuals that are breaking the law or are non-compliant with official obligations and incentivize the right behaviors with the overall goal of improving governance and market order. The Chinese Communist Party intends the social credit score system to “allow the trustworthy to roam freely under heaven while making it hard for the discredited to take a single step.” Even though the system is still under development it currently plays out in real life in myriad ways for private citizens, businesses and government officials. Generally, higher credit scores give people a variety of advantages. Individuals are often given perks such as discounted energy bills and access or better visibility on dating websites. Often, those with higher social credit scores are able to forgo deposits on rental properties, bicycles, and umbrellas. They can even get better travel deals. In addition, Chinese hospitals are currently experimenting with social credit scores. A social credit score above 650 at one hospital allows an individual to see a doctor without lining up to pay.\\n'\n",
    "harvard_law = '\\nIn his important new book, The Schoolhouse Gate: Public Education, the Supreme Court, and the Battle for the American Mind, Professor Justin Driver reminds us that private controversies that arise within the confines of public schools are part of a broader historical arc — one that tracks a range of cultural and intellectual flashpoints in U.S. history. Moreover, Driver explains, these tensions are reflected in constitutional law, and indeed in the history and jurisprudence of the Supreme Court. As such, debates that arise in the context of public education are not simply about the conflict between academic freedom, public safety, and student rights. They mirror our persistent struggle to reconcile our interest in fostering a pluralistic society, rooted in the ideal of individual autonomy, with our desire to cultivate a sense of national unity and shared identity (or, put differently, our effort to reconcile our desire to forge common norms of citizenship with our fear of state indoctrination and overencroachment). In this regard, these debates reflect the unique role that both the school and the courts have played in defining and enforcing the boundaries of American citizenship. \\n'\n",
    "r_digest = '\\nThis week 30 passengers were reportedly injured when a Turkish Airlines flight landing at John F. Kennedy International Airport encountered turbulent conditions. Injuries included bruises, bloody noses, and broken bones. In mid-February, a Delta Airlines flight made an emergency landing to assist three passengers in getting to the nearest hospital after some sudden and unexpected turbulence. Doctors treated 15 passengers after a flight from Miami to Buenos Aires last October for everything from severe bruising to nosebleeds after the plane caught some rough winds over Brazil. In 2016, 23 passengers were injured on a United Airlines flight after severe turbulence threw people into the cabin ceiling. The list goes on. Turbulence has been become increasingly common, with painful outcomes for those on board. And more costly to the airlines, too. Forbes estimates that the cost of turbulence has risen to over $500 million each year in damages and delays. And there are no signs the increase in turbulence will be stopping anytime soon.\\n'\n",
    "time_kids = '\\nThat, of course, is easier said than done. The more you eat salty foods, the more you develop a taste for them. The key to changing your diet is to start small. “Small changes in sodium in foods are not usually noticed,” Quader says. Eventually, she adds, the effort will reset a kid’s taste buds so the salt cravings stop. Bridget Murphy is a dietitian at New York University’s Langone Medical Center. She suggests kids try adding spices to their food instead of salt. Eating fruits and veggies and cutting back on packaged foods will also help. Need a little inspiration? Murphy offers this tip: Focus on the immediate effects of a diet that is high in sodium. High blood pressure can make it difficult to be active. “Do you want to be able to think clearly and perform well in school?” she asks. “If you’re an athlete, do you want to run faster?” If you answered yes to these questions, then it’s time to shake the salt habit.\\n'\n",
    "\n",
    "# Import Textatistic\n",
    "from textatistic import Textatistic\n",
    "\n",
    "# List of excerpts\n",
    "excerpts = [forbes, harvard_law, r_digest, time_kids]\n",
    "\n",
    "# Loop through excerpts and compute index\n",
    "gunning_fog_scores = []\n",
    "flesch_scores = []\n",
    "for excerpt in excerpts:\n",
    "  readability_scores = Textatistic(excerpt).scores\n",
    "  gunning_fog = readability_scores['gunningfog_score']\n",
    "  gunning_fog_scores.append(gunning_fog)\n",
    "  flesch_scores.append(readability_scores['flesch_score'])\n",
    "\n",
    "# Print scores\n",
    "print(gunning_fog_scores)\n",
    "print (flesch_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing techniques\n",
    "- tokenization\n",
    "- lemmatization (base form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Four', 'score', 'and', 'seven', 'years', 'ago', 'our', 'fathers', 'brought', 'forth', 'on', 'this', 'continent', ',', 'a', 'new', 'nation', ',', 'conceived', 'in', 'Liberty', ',', 'and', 'dedicated', 'to', 'the', 'proposition', 'that', 'all', 'men', 'are', 'created', 'equal', '.', 'Now', 'we', \"'re\", 'engaged', 'in', 'a', 'great', 'civil', 'war', ',', 'testing', 'whether', 'that', 'nation', ',', 'or', 'any', 'nation', 'so', 'conceived', 'and', 'so', 'dedicated', ',', 'can', 'long', 'endure', '.', 'We', \"'re\", 'met', 'on', 'a', 'great', 'battlefield', 'of', 'that', 'war', '.', 'We', \"'ve\", 'come', 'to', 'dedicate', 'a', 'portion', 'of', 'that', 'field', ',', 'as', 'a', 'final', 'resting', 'place', 'for', 'those', 'who', 'here', 'gave', 'their', 'lives', 'that', 'that', 'nation', 'might', 'live', '.', 'It', \"'s\", 'altogether', 'fitting', 'and', 'proper', 'that', 'we', 'should', 'do', 'this', '.', 'But', ',', 'in', 'a', 'larger', 'sense', ',', 'we', 'ca', \"n't\", 'dedicate', '-', 'we', 'can', 'not', 'consecrate', '-', 'we', 'can', 'not', 'hallow', '-', 'this', 'ground', '.', 'The', 'brave', 'men', ',', 'living', 'and', 'dead', ',', 'who', 'struggled', 'here', ',', 'have', 'consecrated', 'it', ',', 'far', 'above', 'our', 'poor', 'power', 'to', 'add', 'or', 'detract', '.', 'The', 'world', 'will', 'little', 'note', ',', 'nor', 'long', 'remember', 'what', 'we', 'say', 'here', ',', 'but', 'it', 'can', 'never', 'forget', 'what', 'they', 'did', 'here', '.', 'It', 'is', 'for', 'us', 'the', 'living', ',', 'rather', ',', 'to', 'be', 'dedicated', 'here', 'to', 'the', 'unfinished', 'work', 'which', 'they', 'who', 'fought', 'here', 'have', 'thus', 'far', 'so', 'nobly', 'advanced', '.', 'It', \"'s\", 'rather', 'for', 'us', 'to', 'be', 'here', 'dedicated', 'to', 'the', 'great', 'task', 'remaining', 'before', 'us', '-', 'that', 'from', 'these', 'honored', 'dead', 'we', 'take', 'increased', 'devotion', 'to', 'that', 'cause', 'for', 'which', 'they', 'gave', 'the', 'last', 'full', 'measure', 'of', 'devotion', '-', 'that', 'we', 'here', 'highly', 'resolve', 'that', 'these', 'dead', 'shall', 'not', 'have', 'died', 'in', 'vain', '-', 'that', 'this', 'nation', ',', 'under', 'God', ',', 'shall', 'have', 'a', 'new', 'birth', 'of', 'freedom', '-', 'and', 'that', 'government', 'of', 'the', 'people', ',', 'by', 'the', 'people', ',', 'for', 'the', 'people', ',', 'shall', 'not', 'perish', 'from', 'the', 'earth', '.']\n"
     ]
    }
   ],
   "source": [
    "gettysburg = \"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we're engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We're met on a great battlefield of that war. We've come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It's altogether fitting and proper that we should do this. But, in a larger sense, we can't dedicate - we can not consecrate - we can not hallow - this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It's rather for us to be here dedicated to the great task remaining before us - that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion - that we here highly resolve that these dead shall not have died in vain - that this nation, under God, shall have a new birth of freedom - and that government of the people, by the people, for the people, shall not perish from the earth.\"\n",
    "import spacy\n",
    "\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate the tokens\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['four', 'score', 'and', 'seven', 'year', 'ago', '-PRON-', 'father', 'bring', 'forth', 'on', 'this', 'continent', ',', 'a', 'new', 'nation', ',', 'conceive', 'in', 'Liberty', ',', 'and', 'dedicate', 'to', 'the', 'proposition', 'that', 'all', 'man', 'be', 'create', 'equal', '.', 'now', '-PRON-', 'be', 'engage', 'in', 'a', 'great', 'civil', 'war', ',', 'test', 'whether', 'that', 'nation', ',', 'or', 'any', 'nation', 'so', 'conceive', 'and', 'so', 'dedicated', ',', 'can', 'long', 'endure', '.', '-PRON-', 'be', 'meet', 'on', 'a', 'great', 'battlefield', 'of', 'that', 'war', '.', '-PRON-', 'have', 'come', 'to', 'dedicate', 'a', 'portion', 'of', 'that', 'field', ',', 'as', 'a', 'final', 'resting', 'place', 'for', 'those', 'who', 'here', 'give', '-PRON-', 'life', 'that', 'that', 'nation', 'may', 'live', '.', '-PRON-', 'be', 'altogether', 'fitting', 'and', 'proper', 'that', '-PRON-', 'should', 'do', 'this', '.', 'but', ',', 'in', 'a', 'large', 'sense', ',', '-PRON-', 'can', 'not', 'dedicate', '-', '-PRON-', 'can', 'not', 'consecrate', '-', '-PRON-', 'can', 'not', 'hallow', '-', 'this', 'ground', '.', 'the', 'brave', 'man', ',', 'living', 'and', 'dead', ',', 'who', 'struggle', 'here', ',', 'have', 'consecrate', '-PRON-', ',', 'far', 'above', '-PRON-', 'poor', 'power', 'to', 'add', 'or', 'detract', '.', 'the', 'world', 'will', 'little', 'note', ',', 'nor', 'long', 'remember', 'what', '-PRON-', 'say', 'here', ',', 'but', '-PRON-', 'can', 'never', 'forget', 'what', '-PRON-', 'do', 'here', '.', '-PRON-', 'be', 'for', '-PRON-', 'the', 'living', ',', 'rather', ',', 'to', 'be', 'dedicate', 'here', 'to', 'the', 'unfinished', 'work', 'which', '-PRON-', 'who', 'fight', 'here', 'have', 'thus', 'far', 'so', 'nobly', 'advanced', '.', '-PRON-', 'be', 'rather', 'for', '-PRON-', 'to', 'be', 'here', 'dedicate', 'to', 'the', 'great', 'task', 'remain', 'before', '-PRON-', '-', 'that', 'from', 'these', 'honor', 'dead', '-PRON-', 'take', 'increase', 'devotion', 'to', 'that', 'cause', 'for', 'which', '-PRON-', 'give', 'the', 'last', 'full', 'measure', 'of', 'devotion', '-', 'that', '-PRON-', 'here', 'highly', 'resolve', 'that', 'these', 'dead', 'shall', 'not', 'have', 'die', 'in', 'vain', '-', 'that', 'this', 'nation', ',', 'under', 'God', ',', 'shall', 'have', 'a', 'new', 'birth', 'of', 'freedom', '-', 'and', 'that', 'government', 'of', 'the', 'people', ',', 'by', 'the', 'people', ',', 'for', 'the', 'people', ',', 'shall', 'not', 'perish', 'from', 'the', 'earth', '.']\n"
     ]
    }
   ],
   "source": [
    "# Create a Doc object\n",
    "doc = nlp(gettysburg)\n",
    "\n",
    "# Generate lemmas\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "print (lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four score and seven year ago -PRON- father bring forth on this continent , a new nation , conceive in Liberty , and dedicate to the proposition that all man be create equal . now -PRON- be engage in a great civil war , test whether that nation , or any nation so conceive and so dedicated , can long endure . -PRON- be meet on a great battlefield of that war . -PRON- have come to dedicate a portion of that field , as a final resting place for those who here give -PRON- life that that nation may live . -PRON- be altogether fitting and proper that -PRON- should do this . but , in a large sense , -PRON- can not dedicate - -PRON- can not consecrate - -PRON- can not hallow - this ground . the brave man , living and dead , who struggle here , have consecrate -PRON- , far above -PRON- poor power to add or detract . the world will little note , nor long remember what -PRON- say here , but -PRON- can never forget what -PRON- do here . -PRON- be for -PRON- the living , rather , to be dedicate here to the unfinished work which -PRON- who fight here have thus far so nobly advanced . -PRON- be rather for -PRON- to be here dedicate to the great task remain before -PRON- - that from these honor dead -PRON- take increase devotion to that cause for which -PRON- give the last full measure of devotion - that -PRON- here highly resolve that these dead shall not have die in vain - that this nation , under God , shall have a new birth of freedom - and that government of the people , by the people , for the people , shall not perish from the earth .\n"
     ]
    }
   ],
   "source": [
    "# Convert lemmas into a string\n",
    "print(' '.join(lemmas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Twenty-first-century politics has witnessed an alarming rise of populism in the U.S. and Europe. The first warning signs came with the UK Brexit Referendum vote in 2016 swinging in the way of Leave. This was followed by a stupendous victory by billionaire Donald Trump to become the 45th President of the United States in November 2016. Since then, Europe has seen a steady rise in populist and far-right parties that have capitalized on Europe’s Immigration Crisis to raise nationalist and anti-Europe sentiments. Some instances include Alternative for Germany (AfD) winning 12.6% of all seats and entering the Bundestag, thus upsetting Germany’s political order for the first time since the Second World War, the success of the Five Star Movement in Italy and the surge in popularity of neo-nazism and neo-fascism in countries such as Hungary, Czech Republic, Poland and Austria.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Text cleaning\n",
    "blog = '\\nTwenty-first-century politics has witnessed an alarming rise of populism in the U.S. and Europe. The first warning signs came with the UK Brexit Referendum vote in 2016 swinging in the way of Leave. This was followed by a stupendous victory by billionaire Donald Trump to become the 45th President of the United States in November 2016. Since then, Europe has seen a steady rise in populist and far-right parties that have capitalized on Europe’s Immigration Crisis to raise nationalist and anti-Europe sentiments. Some instances include Alternative for Germany (AfD) winning 12.6% of all seats and entering the Bundestag, thus upsetting Germany’s political order for the first time since the Second World War, the success of the Five Star Movement in Italy and the surge in popularity of neo-nazism and neo-fascism in countries such as Hungary, Czech Republic, Poland and Austria.\\n'\n",
    "print (blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "century politic witness alarming rise populism Europe warning sign come UK Brexit Referendum vote swinging way Leave follow stupendous victory billionaire Donald Trump President United States November Europe steady rise populist far right party capitalize Europe Immigration Crisis raise nationalist anti europe sentiment instance include alternative Germany AfD win seat enter Bundestag upset Germany political order time Second World War success Star Movement Italy surge popularity neo nazism neo fascism country Hungary Czech Republic Poland Austria\n"
     ]
    }
   ],
   "source": [
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(blog)\n",
    "\n",
    "# Generate lemmatized tokens\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "\n",
    "# Remove stopwords and non-alphabetic tokens\n",
    "a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "\n",
    "# Print string after text cleaning\n",
    "print(' '.join(a_lemmas))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "  \t# Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "# Apply preprocess to ted['transcript']\n",
    "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
    "print(ted['transcript'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\n', 'SPACE'), ('Twenty', 'NUM'), ('-', 'PUNCT'), ('first', 'ADJ'), ('-', 'PUNCT'), ('century', 'NOUN'), ('politics', 'NOUN'), ('has', 'AUX'), ('witnessed', 'VERB'), ('an', 'DET'), ('alarming', 'ADJ'), ('rise', 'NOUN'), ('of', 'ADP'), ('populism', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('U.S.', 'PROPN'), ('and', 'CCONJ'), ('Europe', 'PROPN'), ('.', 'PUNCT'), ('The', 'DET'), ('first', 'ADJ'), ('warning', 'NOUN'), ('signs', 'NOUN'), ('came', 'VERB'), ('with', 'ADP'), ('the', 'DET'), ('UK', 'PROPN'), ('Brexit', 'PROPN'), ('Referendum', 'PROPN'), ('vote', 'NOUN'), ('in', 'ADP'), ('2016', 'NUM'), ('swinging', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('way', 'NOUN'), ('of', 'ADP'), ('Leave', 'PROPN'), ('.', 'PUNCT'), ('This', 'DET'), ('was', 'AUX'), ('followed', 'VERB'), ('by', 'ADP'), ('a', 'DET'), ('stupendous', 'ADJ'), ('victory', 'NOUN'), ('by', 'ADP'), ('billionaire', 'NOUN'), ('Donald', 'PROPN'), ('Trump', 'PROPN'), ('to', 'PART'), ('become', 'VERB'), ('the', 'DET'), ('45th', 'ADJ'), ('President', 'PROPN'), ('of', 'ADP'), ('the', 'DET'), ('United', 'PROPN'), ('States', 'PROPN'), ('in', 'ADP'), ('November', 'PROPN'), ('2016', 'NUM'), ('.', 'PUNCT'), ('Since', 'SCONJ'), ('then', 'ADV'), (',', 'PUNCT'), ('Europe', 'PROPN'), ('has', 'AUX'), ('seen', 'VERB'), ('a', 'DET'), ('steady', 'ADJ'), ('rise', 'NOUN'), ('in', 'ADP'), ('populist', 'NOUN'), ('and', 'CCONJ'), ('far', 'ADJ'), ('-', 'PUNCT'), ('right', 'ADJ'), ('parties', 'NOUN'), ('that', 'DET'), ('have', 'AUX'), ('capitalized', 'VERB'), ('on', 'ADP'), ('Europe', 'PROPN'), ('’s', 'PART'), ('Immigration', 'PROPN'), ('Crisis', 'PROPN'), ('to', 'PART'), ('raise', 'VERB'), ('nationalist', 'ADJ'), ('and', 'CCONJ'), ('anti', 'ADJ'), ('-', 'ADJ'), ('Europe', 'ADJ'), ('sentiments', 'NOUN'), ('.', 'PUNCT'), ('Some', 'DET'), ('instances', 'NOUN'), ('include', 'VERB'), ('Alternative', 'NOUN'), ('for', 'ADP'), ('Germany', 'PROPN'), ('(', 'PUNCT'), ('AfD', 'PROPN'), (')', 'PUNCT'), ('winning', 'VERB'), ('12.6', 'NUM'), ('%', 'NOUN'), ('of', 'ADP'), ('all', 'DET'), ('seats', 'NOUN'), ('and', 'CCONJ'), ('entering', 'VERB'), ('the', 'DET'), ('Bundestag', 'PROPN'), (',', 'PUNCT'), ('thus', 'ADV'), ('upsetting', 'VERB'), ('Germany', 'PROPN'), ('’s', 'PART'), ('political', 'ADJ'), ('order', 'NOUN'), ('for', 'ADP'), ('the', 'DET'), ('first', 'ADJ'), ('time', 'NOUN'), ('since', 'SCONJ'), ('the', 'DET'), ('Second', 'PROPN'), ('World', 'PROPN'), ('War', 'PROPN'), (',', 'PUNCT'), ('the', 'DET'), ('success', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('Five', 'NUM'), ('Star', 'PROPN'), ('Movement', 'PROPN'), ('in', 'ADP'), ('Italy', 'PROPN'), ('and', 'CCONJ'), ('the', 'DET'), ('surge', 'NOUN'), ('in', 'ADP'), ('popularity', 'NOUN'), ('of', 'ADP'), ('neo', 'PROPN'), ('-', 'PUNCT'), ('nazism', 'PROPN'), ('and', 'CCONJ'), ('neo', 'PROPN'), ('-', 'PUNCT'), ('fascism', 'PROPN'), ('in', 'ADP'), ('countries', 'NOUN'), ('such', 'ADJ'), ('as', 'SCONJ'), ('Hungary', 'PROPN'), (',', 'PUNCT'), ('Czech', 'PROPN'), ('Republic', 'PROPN'), (',', 'PUNCT'), ('Poland', 'PROPN'), ('and', 'CCONJ'), ('Austria', 'PROPN'), ('.', 'PUNCT'), ('\\n', 'SPACE')]\n"
     ]
    }
   ],
   "source": [
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print (pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He found himself understanding the wearisomeness of this life, where every path was an improvisation and a considerable part of one’s waking life was spent watching one’s feet.\n"
     ]
    }
   ],
   "source": [
    "lotf = 'He found himself understanding the wearisomeness of this life, where every path was an improvisation and a considerable part of one’s waking life was spent watching one’s feet.'\n",
    "print (lotf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRON'), ('found', 'VERB'), ('himself', 'PRON'), ('understanding', 'VERB'), ('the', 'DET'), ('wearisomeness', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('life', 'NOUN'), (',', 'PUNCT'), ('where', 'ADV'), ('every', 'DET'), ('path', 'NOUN'), ('was', 'AUX'), ('an', 'DET'), ('improvisation', 'NOUN'), ('and', 'CCONJ'), ('a', 'DET'), ('considerable', 'ADJ'), ('part', 'NOUN'), ('of', 'ADP'), ('one', 'NUM'), ('’s', 'PART'), ('waking', 'VERB'), ('life', 'NOUN'), ('was', 'AUX'), ('spent', 'VERB'), ('watching', 'VERB'), ('one', 'PRON'), ('’s', 'PART'), ('feet', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "# Create a Doc object\n",
    "doc = nlp(lotf)\n",
    "\n",
    "# Generate tokens and pos tags\n",
    "pos = [(token.text, token.pos_) for token in doc]\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Returns number of proper nouns\n",
    "def proper_nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of proper nouns\n",
    "    return pos.count('PROPN')\n",
    "\n",
    "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Returns number of nouns\n",
    "def nouns(text, model=nlp):\n",
    "  \t# Create doc object\n",
    "    doc = model(text)\n",
    "    # Generate list of POS tags\n",
    "    pos = [token.pos_ for token in doc]\n",
    "    \n",
    "    # Return number of nouns\n",
    "    return pos.count('NOUN')\n",
    "\n",
    "print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp)) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
    "headlines['num_noun'] = headlines['title'].apply(nouns)\n",
    "\n",
    "# Compute mean of proper nouns\n",
    "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
    "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
    "\n",
    "# Compute mean of other nouns\n",
    "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
    "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))\n",
    "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER:\n",
    "- efficient search algorithms\n",
    "- question answering\n",
    "- customer service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sundar Pichai PERSON\n",
      "Google ORG\n",
      "Mountain View GPE\n"
     ]
    }
   ],
   "source": [
    "# named entities\n",
    "# Load the required model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Create a Doc instance \n",
    "text = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print all named entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It’s' been a busy day for Facebook  exec op-eds. Earlier this morning, Sheryl Sandberg broke the site’s silence around the Christchurch massacre, and now Mark Zuckerberg is calling on governments and other bodies to increase regulation around the sorts of data Facebook traffics in. He’s hoping to get out in front of heavy-handed regulation and get a seat at the table shaping it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tc = \"\\nIt’s' been a busy day for Facebook  exec op-eds. Earlier this morning, Sheryl Sandberg broke the site’s silence around the Christchurch massacre, and now Mark Zuckerberg is calling on governments and other bodies to increase regulation around the sorts of data Facebook traffics in. He’s hoping to get out in front of heavy-handed regulation and get a seat at the table shaping it.\\n\"\n",
    "print (tc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sheryl Sandberg', 'Mark Zuckerberg']\n"
     ]
    }
   ],
   "source": [
    "def find_persons(text):\n",
    "  # Create Doc object\n",
    "  doc = nlp(text)\n",
    "  \n",
    "  # Identify the persons\n",
    "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "  \n",
    "  # Return persons\n",
    "  return persons\n",
    "\n",
    "print(find_persons(tc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   an  decade  endangered  have  is  jungle  king  lifespans  lion  lions  of  \\\n",
      "0   0       0           0     0   1       1     1          0     1      0   1   \n",
      "1   0       1           0     1   0       0     0          1     0      1   1   \n",
      "2   1       0           1     0   1       0     0          0     1      0   0   \n",
      "\n",
      "   species  the  \n",
      "0        0    3  \n",
      "1        0    0  \n",
      "2        1    1  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "corpus = ['The lion is the king of the jungle', 'Lions have lifespans of a decade', 'The lion is an endangered species']\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert bow_matrix into a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
    "\n",
    "# Map the column names to vocabulary \n",
    "bow_df.columns = vectorizer.get_feature_names()\n",
    "\n",
    "# Print bow_df\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams:\n",
    "- Bag of words (BoW) -> context is lost\n",
    "- use contiguous sequence of n elements\n",
    "- used in sentence completion, spelling correction, translation correction\n",
    "- curse of dimensionality is a shortcoming "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Generate n-grams upto n=1\n",
    "vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n",
    "ng1 = vectorizer_ng1.fit_transform(corpus)\n",
    "\n",
    "# Define an instance of MultinomialNB \n",
    "clf_ng = MultinomialNB()\n",
    "\n",
    "# Fit the classifier \n",
    "clf_ng.fit(X_train_ng, y_train)\n",
    "\n",
    "# Measure the accuracy \n",
    "accuracy = clf_ng.score(X_test_ng, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was not good. The plot had several holes and the acting lacked panache.\"\n",
    "prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation System:\n",
    "- text preprocessing\n",
    "- generate tf-idf vectors\n",
    "- generate cosine similarity matrix\n",
    "- recommender function (outputs titles with highest scores, ignores score of 1 (itself))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(movie_plots)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    " \n",
    "# Generate recommendations \n",
    "print(get_recommendations('The Dark Knight Rises', cosine_sim, indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Led by Woody, Andy's toys live happily in his ...\n",
      "1       When siblings Judy and Peter discover an encha...\n",
      "2       A family wedding reignites the ancient feud be...\n",
      "3       Cheated on, mistreated and stepped on, the wom...\n",
      "4       Just when George Banks has recovered from his ...\n",
      "                              ...                        \n",
      "9094    A man must cope with the loss of his wife and ...\n",
      "9095    Rustom Pavri, an honourable officer of the Ind...\n",
      "9096    Village lad Sarman is drawn to big, bad Mohenj...\n",
      "9097    From the mind behind Evangelion comes a hit la...\n",
      "9098    The band stormed Europe in 1963, and, in 1964,...\n",
      "Name: overview, Length: 9099, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"movie_overviews.csv\")\n",
    "movie_plots = df[\"overview\"]\n",
    "print (movie_plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = df.drop(columns=[\"id\", \"overview\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "\n",
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get index of movie that matches title\n",
    "    idx = indices[title]\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132                              Batman Forever\n",
      "6907                            The Dark Knight\n",
      "1116                             Batman Returns\n",
      "7573                 Batman: Under the Red Hood\n",
      "524                                      Batman\n",
      "7907                           Batman: Year One\n",
      "8171    Batman: The Dark Knight Returns, Part 1\n",
      "2581               Batman: Mask of the Phantasm\n",
      "8232    Batman: The Dark Knight Returns, Part 2\n",
      "6150                              Batman Begins\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(movie_plots.values.astype('U'))\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    " \n",
    "# Generate recommendations \n",
    "print(get_recommendations('The Dark Knight Rises', cosine_sim, indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             title  \\\n",
      "0      Do schools kill creativity?   \n",
      "1      Averting the climate crisis   \n",
      "2                 Simplicity sells   \n",
      "3              Greening the ghetto   \n",
      "4  The best stats you've ever seen   \n",
      "\n",
      "                                         description  \n",
      "0  Sir Ken Robinson makes an entertaining and pro...  \n",
      "1  With the same humor and humanity he exuded in ...  \n",
      "2  New York Times columnist David Pogue takes aim...  \n",
      "3  In an emotionally charged talk, MacArthur-winn...  \n",
      "4  You've never seen data presented like this. Wi...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"ted_main.csv\")\n",
    "df = df[['title','description']]\n",
    "print (df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    idx = indices[title]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return df['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535                           Life lessons from an ad man\n",
      "905                         Everyday compassion at Google\n",
      "2430                           We should all be feminists\n",
      "2275                               The new American Dream\n",
      "395                   Why not make video games for girls?\n",
      "2002    The art of first impressions -- in design and ...\n",
      "1216                             Gaming for understanding\n",
      "1659                        So we leaned in ... now what?\n",
      "1134                  The secret structure of great talks\n",
      "1459         The mind behind Tesla, SpaceX, SolarCity ...\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "desc = df['description']\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(desc)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix)\n",
    "\n",
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(df.index, index=df['title']).drop_duplicates()\n",
    " \n",
    "# Generate recommendations \n",
    "print(get_recommendations(\"5 ways to kill your dreams\", cosine_sim, indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings\n",
    "- mapping words into an n-dimensional vector space\n",
    "- produced using deep learning and huge amounts of data\n",
    "- discern how similar two words are to each other\n",
    "- used to detect synonyms and antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.41468892  3.669314    1.8560282  -2.3108435   2.8852868  -0.82837206\n",
      "  4.0181217   3.530494    5.4953594   0.18829852  0.63895303  3.6075087\n",
      " -2.1611521   2.1002114   2.4749072  -0.6116394  -2.7477045   1.9441204\n",
      " -2.64507    -0.7410308  -1.7187613  -1.9501854  -2.730586   -4.0733533\n",
      " -2.9703767  -2.308702   -4.689859   -1.6497633  -0.8990534   0.23329073\n",
      "  1.9284855  -1.8175116  -2.450923    3.0092754   6.3828716   1.2543191\n",
      " -2.3071318   4.477075    2.3773994  -0.5180781   1.8429365  -0.7347867\n",
      " -2.050178   -2.0417776   0.8836121  -0.6362535   0.5782076  -1.6812638\n",
      " -2.5833106   5.29397    -1.9922516  -2.9236636  -0.30355528  2.6853185\n",
      " -0.5162385   1.0921136   3.2269735  -0.47499296  1.9768519  -2.3588443\n",
      "  3.1970434  -1.0606685  -1.3306926   5.1694493  -0.89199847  0.6138899\n",
      "  0.26201618 -3.8502092  -3.6942177   1.5612483  -2.991671    0.45248574\n",
      " -1.2816932  -0.12692854  2.1817818  -1.9396955   1.2162955  -1.6497872\n",
      "  0.6531148  -1.421446   -3.1822352   2.8020358  -5.6519575   5.558767\n",
      "  3.7175133   3.7971978   3.1989615  -1.0721521   1.0002325   0.27706388\n",
      " -3.4716053  -3.907165   -0.8853098  -1.4243131  -0.620152   -0.00950694]\n",
      "[-1.9767625  -1.4894304  -3.3289816  -1.3716142  -3.2040567   2.6016989\n",
      " -0.9700591   3.4772263   1.6296268   0.9686447   0.0511328  -2.399566\n",
      "  0.4590459   4.810575    0.72386885  2.0719333  -0.85090107 -1.7887536\n",
      "  2.9955122  -0.5807384  -0.39856344  3.7126117  -2.0972176   1.4481251\n",
      "  0.85736066  0.10309422  2.5661206  -5.3951435   1.7790506   2.8276827\n",
      "  4.3614507  -1.5599267   5.173158   -1.0325582  -2.1292515  -2.5062726\n",
      "  0.9864334  -2.1336107   0.14355242 -2.022987   -1.7026011   3.044218\n",
      "  1.4122972  -3.3784833   2.6669955   1.8592342  -0.8083391   1.3511584\n",
      " -1.3926289  -2.0544422   1.4830731  -4.2131314   4.018028   -1.6249415\n",
      " -0.38581067 -2.8884306  -0.32927758  0.18311346 -0.79053855  4.3551273\n",
      "  6.565252    0.5548755  -0.13484651  0.1536095   0.6712878   0.622223\n",
      " -1.7625964  -2.048388    1.515177   -3.6825392  -1.499192    4.11914\n",
      " -0.07228613  2.8501594  -0.539696   -1.2631454  -1.8525157   0.99859536\n",
      "  0.9515454  -2.9489574  -0.92220604  0.0532791  -1.7352929  -4.668089\n",
      "  1.0206437   0.27338272  1.1072277  -1.8434695  -1.1637287   1.5892518\n",
      " -2.835981   -2.2396135  -1.4735882   1.0598631  -2.0557709   2.760159  ]\n",
      "[ 6.076031   -3.559865    0.17833847 -1.5397906   7.3471665  -2.04406\n",
      "  3.8094857  -0.71997344 -0.39070934 -1.9333154   0.4205354  -2.3212535\n",
      "  0.45025074 -0.40548083 -2.3996377   1.7735667  -1.082812    1.209753\n",
      " -1.8279647  -2.6941125  -0.9446949  -2.2207336  -1.2812045  -1.098181\n",
      "  4.1793685  -1.6863046  -1.626112   -1.6470995   3.2586703  -0.31841063\n",
      " -1.5740604  -2.7806053   0.15729356  2.5092244  -0.0177705  -1.9517946\n",
      "  3.823415   -0.9494051  -4.1650863  -1.0456954  -1.7399969  -0.51687264\n",
      " -1.0262958  -0.4179983   2.3024259  -0.12827128 -0.44308615  1.0180118\n",
      " -1.645323    0.6083437   1.1990676   5.2626486  -2.6516461  -2.805363\n",
      " -1.2215571  -0.42999467  0.5752727   3.342877    0.6572639   0.21806306\n",
      "  0.2774682   0.08994734 -0.53776187 -1.3317866   4.1727366  -4.724384\n",
      " -0.45447773 -0.6454975  -0.2519518   2.6467283  -3.133453    0.48211375\n",
      "  2.4084642  -0.27175534 -1.1791025  -2.7832665   1.4759936  -1.7619511\n",
      " -0.51856315  3.9060206  -1.3234742  -1.0355473   1.6990674  -2.5336382\n",
      "  2.5275793   0.7265431   2.093513   -1.9505513   0.71167076  0.35525396\n",
      "  1.8946981   1.1413212   1.0359731   1.6812512   0.47876835  5.6103826 ]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('I am happy')\n",
    "for token in doc:\n",
    "    print (token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy happy 1.0\n",
      "happy joyous 0.49316472\n",
      "happy sad 0.21885948\n",
      "joyous happy 0.49316472\n",
      "joyous joyous 1.0\n",
      "joyous sad 0.45893645\n",
      "sad happy 0.21885948\n",
      "sad joyous 0.45893645\n",
      "sad sad 1.0\n"
     ]
    }
   ],
   "source": [
    "# word similarities\n",
    "doc = nlp(\"happy joyous sad\")\n",
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print (token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5389563304046386\n",
      "0.6807781075992245\n"
     ]
    }
   ],
   "source": [
    "# document similarities\n",
    "sent1 = nlp(\"I am happy\")\n",
    "sent2 = nlp(\"I am joyously amazingly ok\")\n",
    "sent3 = nlp(\"I am not sad\")\n",
    "print (sent1.similarity(sent2))\n",
    "print (sent1.similarity(sent3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6172701038107117\n",
      "0.9102045307629318\n"
     ]
    }
   ],
   "source": [
    "hopes = \"\\nBeyond the horizon of the place we lived when we were young\\nIn a world of magnets and miracles\\nOur thoughts strayed constantly and without boundary\\nThe ringing of the division bell had begun\\nAlong the Long Road and on down the Causeway\\nDo they still meet there by the Cut\\nThere was a ragged band that followed in our footsteps\\nRunning before times took our dreams away\\nLeaving the myriad small creatures trying to tie us to the ground\\nTo a life consumed by slow decay\\nThe grass was greener\\nThe light was brighter\\nWhen friends surrounded\\nThe nights of wonder\\nLooking beyond the embers of bridges glowing behind us\\nTo a glimpse of how green it was on the other side\\nSteps taken forwards but sleepwalking back again\\nDragged by the force of some in a tide\\nAt a higher altitude with flag unfurled\\nWe reached the dizzy heights of that dreamed of world\\nEncumbered forever by desire and ambition\\nThere's a hunger still unsatisfied\\nOur weary eyes still stray to the horizon\\nThough down this road we've been so many times\\nThe grass was greener\\nThe light was brighter\\nThe taste was sweeter\\nThe nights of wonder\\nWith friends surrounded\\nThe dawn mist glowing\\nThe water flowing\\nThe endless river\\nForever and ever\\n\"\n",
    "hey = \"\\nHey you, out there in the cold\\nGetting lonely, getting old\\nCan you feel me?\\nHey you, standing in the aisles\\nWith itchy feet and fading smiles\\nCan you feel me?\\nHey you, don't help them to bury the light\\nDon't give in without a fight\\nHey you out there on your own\\nSitting naked by the phone\\nWould you touch me?\\nHey you with you ear against the wall\\nWaiting for someone to call out\\nWould you touch me?\\nHey you, would you help me to carry the stone?\\nOpen your heart, I'm coming home\\nBut it was only fantasy\\nThe wall was too high\\nAs you can see\\nNo matter how he tried\\nHe could not break free\\nAnd the worms ate into his brain\\nHey you, out there on the road\\nAlways doing what you're told\\nCan you help me?\\nHey you, out there beyond the wall\\nBreaking bottles in the hall\\nCan you help me?\\nHey you, don't tell me there's no hope at all\\nTogether we stand, divided we fall\\n\"\n",
    "mother = \"\\nMother do you think they'll drop the bomb?\\nMother do you think they'll like this song?\\nMother do you think they'll try to break my balls?\\nOoh, ah\\nMother should I build the wall?\\nMother should I run for President?\\nMother should I trust the government?\\nMother will they put me in the firing mine?\\nOoh ah,\\nIs it just a waste of time?\\nHush now baby, baby, don't you cry.\\nMama's gonna make all your nightmares come true.\\nMama's gonna put all her fears into you.\\nMama's gonna keep you right here under her wing.\\nShe won't let you fly, but she might let you sing.\\nMama's gonna keep baby cozy and warm.\\nOoh baby, ooh baby, ooh baby,\\nOf course mama's gonna help build the wall.\\nMother do you think she's good enough, for me?\\nMother do you think she's dangerous, to me?\\nMother will she tear your little boy apart?\\nOoh ah,\\nMother will she break my heart?\\nHush now baby, baby don't you cry.\\nMama's gonna check out all your girlfriends for you.\\nMama won't let anyone dirty get through.\\nMama's gonna wait up until you get in.\\nMama will always find out where you've been.\\nMama's gonna keep baby healthy and clean.\\nOoh baby, ooh baby, ooh baby,\\nYou'll always be baby to me.\\nMother, did it need to be so high?\\n\"\n",
    "# Create Doc objects\n",
    "mother_doc = nlp(mother)\n",
    "hopes_doc = nlp(hopes)\n",
    "hey_doc = nlp(hey)\n",
    "\n",
    "# Print similarity between mother and hopes\n",
    "print(mother_doc.similarity(hopes_doc))\n",
    "\n",
    "# Print similarity between mother and hey\n",
    "print(mother_doc.similarity(hey_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review:\n",
    "- basic features\n",
    "- readability scores\n",
    "- tokenization and lemmatization\n",
    "- text cleaning\n",
    "- POS tagging, NER\n",
    "- n-gram modeling\n",
    "- tf-idf\n",
    "- cosine similarity\n",
    "- word embeddings using spacy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
